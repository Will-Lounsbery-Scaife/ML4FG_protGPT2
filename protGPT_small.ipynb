{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GBNUfMJMn2N2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698519575842,"user_tz":240,"elapsed":28171,"user":{"displayName":"wfls96","userId":"04099767878386371096"}},"outputId":"27662668-3976-4a5f-b697-24fb5061aa6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: dill, multiprocess, huggingface-hub, datasets\n","Successfully installed datasets-2.14.6 dill-0.3.7 huggingface-hub-0.18.0 multiprocess-0.70.15\n","Collecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.6)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Installing collected packages: responses, evaluate\n","Successfully installed evaluate-0.4.1 responses-0.18.0\n","Collecting accelerate\n","  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.18.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.24.0\n","Collecting transformers\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.18.0\n","    Uninstalling huggingface-hub-0.18.0:\n","      Successfully uninstalled huggingface-hub-0.18.0\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"]}],"source":["!pip install datasets\n","!pip install evaluate\n","!pip install accelerate -U\n","!pip install transformers"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"xKkRV1q6n7H8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698519583187,"user_tz":240,"elapsed":7349,"user":{"displayName":"wfls96","userId":"04099767878386371096"}},"outputId":"1f042245-49d1-4b86-e31f-9c647ee01754"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!python '/content/drive/MyDrive/Colabs/run_clm.py' --model_name_or_path nferruz/ProtGPT2 \\\n","    --train_file '/content/drive/MyDrive/Colabs/training_small.txt' \\\n","    --validation_file '/content/drive/MyDrive/Colabs/validation_small.txt' \\\n","    --tokenizer_name nferruz/ProtGPT2 \\\n","    --do_train \\\n","    --do_eval  \\\n","    --output_dir '/content/drive/MyDrive/Colabs/result1' \\\n","    --overwrite_output_dir \\\n","    --per_device_train_batch_size 1 \\\n","    --per_device_eval_batch_size 1 \\\n","    --gradient_accumulation_steps=16 \\\n","    --fp16 \\\n","    --learning_rate 1e-06 \\\n","    --save_steps 800 \\\n","    --save_total_limit 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWi8ZEIGpFVv","executionInfo":{"status":"ok","timestamp":1698535235295,"user_tz":240,"elapsed":15626124,"user":{"displayName":"wfls96","userId":"04099767878386371096"}},"outputId":"c4c0f1ff-d537-41f2-82b8-a72a025fc1d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-10-28 19:00:15.939255: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-10-28 19:00:15.939315: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-10-28 19:00:15.939344: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-10-28 19:00:17.078477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1698519620.202376    1944 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n","10/28/2023 19:00:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n","10/28/2023 19:00:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=True,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=16,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=1e-06,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Colabs/result1/runs/Oct28_19-00-20_d6cc36b03fe1,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/Colabs/result1,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=1,\n","per_device_train_batch_size=1,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/Colabs/result1,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=800,\n","save_strategy=steps,\n","save_total_limit=2,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-1d41c0d41954827a\n","10/28/2023 19:00:26 - INFO - datasets.builder - Using custom data configuration default-1d41c0d41954827a\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n","10/28/2023 19:00:26 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n","Generating dataset text (/root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)\n","10/28/2023 19:00:26 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)\n","Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34...\n","10/28/2023 19:00:26 - INFO - datasets.builder - Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34...\n","Downloading data files: 100% 2/2 [00:00<00:00, 4132.32it/s]\n","Downloading took 0.0 min\n","10/28/2023 19:00:26 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","10/28/2023 19:00:26 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:04<00:00,  2.31s/it]\n","Generating train split\n","10/28/2023 19:00:31 - INFO - datasets.builder - Generating train split\n","Generating train split: 3508311 examples [00:03, 1130891.51 examples/s]\n","Generating validation split\n","10/28/2023 19:00:34 - INFO - datasets.builder - Generating validation split\n","Generating validation split: 618801 examples [00:01, 365818.92 examples/s]\n","Unable to verify splits sizes.\n","10/28/2023 19:00:36 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34. Subsequent calls will reuse this data.\n","10/28/2023 19:00:36 - INFO - datasets.builder - Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34. Subsequent calls will reuse this data.\n","Downloading (…)lve/main/config.json: 100% 850/850 [00:00<00:00, 4.30MB/s]\n","[INFO|configuration_utils.py:715] 2023-10-28 19:00:36,734 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/config.json\n","[INFO|configuration_utils.py:775] 2023-10-28 19:00:36,735 >> Model config GPT2Config {\n","  \"_name_or_path\": \"nferruz/ProtGPT2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 0,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1280,\n","  \"n_head\": 20,\n","  \"n_inner\": null,\n","  \"n_layer\": 36,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.34.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","[INFO|tokenization_auto.py:550] 2023-10-28 19:00:36,985 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:715] 2023-10-28 19:00:37,276 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/config.json\n","[INFO|configuration_utils.py:775] 2023-10-28 19:00:37,277 >> Model config GPT2Config {\n","  \"_name_or_path\": \"nferruz/ProtGPT2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 0,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1280,\n","  \"n_head\": 20,\n","  \"n_inner\": null,\n","  \"n_layer\": 36,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.34.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","Downloading (…)olve/main/vocab.json: 100% 655k/655k [00:00<00:00, 1.43MB/s]\n","Downloading (…)olve/main/merges.txt: 100% 314k/314k [00:00<00:00, 51.5MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 1.07M/1.07M [00:00<00:00, 4.53MB/s]\n","Downloading (…)cial_tokens_map.json: 100% 357/357 [00:00<00:00, 2.15MB/s]\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 19:00:41,550 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/vocab.json\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 19:00:41,550 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/merges.txt\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 19:00:41,550 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/tokenizer.json\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 19:00:41,550 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 19:00:41,550 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 19:00:41,550 >> loading file tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:715] 2023-10-28 19:00:41,550 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/config.json\n","[INFO|configuration_utils.py:775] 2023-10-28 19:00:41,551 >> Model config GPT2Config {\n","  \"_name_or_path\": \"nferruz/ProtGPT2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 0,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1280,\n","  \"n_head\": 20,\n","  \"n_inner\": null,\n","  \"n_layer\": 36,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.34.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","Downloading pytorch_model.bin: 100% 3.13G/3.13G [00:09<00:00, 328MB/s]\n","[INFO|modeling_utils.py:2993] 2023-10-28 19:00:52,024 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/pytorch_model.bin\n","[INFO|configuration_utils.py:770] 2023-10-28 19:00:53,711 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 0\n","}\n","\n","[INFO|modeling_utils.py:3775] 2023-10-28 19:01:01,487 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","[INFO|modeling_utils.py:3783] 2023-10-28 19:01:01,487 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at nferruz/ProtGPT2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","[INFO|modeling_utils.py:3352] 2023-10-28 19:01:01,737 >> Generation config file not found, using a generation config created from the model config.\n","Running tokenizer on dataset:   0% 0/3508311 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-ab081c8bbbd29204.arrow\n","10/28/2023 19:01:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-ab081c8bbbd29204.arrow\n","Running tokenizer on dataset: 100% 3508311/3508311 [01:05<00:00, 53793.67 examples/s]\n","Running tokenizer on dataset:   0% 0/618801 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-332caaa9ba9261ca.arrow\n","10/28/2023 19:02:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-332caaa9ba9261ca.arrow\n","Running tokenizer on dataset: 100% 618801/618801 [00:11<00:00, 53748.06 examples/s]\n","10/28/2023 19:02:18 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Using block_size=1024 instead. You can change that default value by passing --block_size xxx.\n","Grouping texts in chunks of 1024:   0% 0/3508311 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-19cc271c971a97fa.arrow\n","10/28/2023 19:02:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-19cc271c971a97fa.arrow\n","Grouping texts in chunks of 1024: 100% 3508311/3508311 [01:06<00:00, 52764.90 examples/s]\n","Grouping texts in chunks of 1024:   0% 0/618801 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-74f973b5d0ce3073.arrow\n","10/28/2023 19:03:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-74f973b5d0ce3073.arrow\n","Grouping texts in chunks of 1024: 100% 618801/618801 [00:11<00:00, 52521.55 examples/s]\n","Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 16.3MB/s]\n","[INFO|trainer.py:1760] 2023-10-28 19:03:40,112 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-28 19:03:40,112 >>   Num examples = 43,907\n","[INFO|trainer.py:1762] 2023-10-28 19:03:40,112 >>   Num Epochs = 3\n","[INFO|trainer.py:1763] 2023-10-28 19:03:40,112 >>   Instantaneous batch size per device = 1\n","[INFO|trainer.py:1766] 2023-10-28 19:03:40,112 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1767] 2023-10-28 19:03:40,112 >>   Gradient Accumulation steps = 16\n","[INFO|trainer.py:1768] 2023-10-28 19:03:40,112 >>   Total optimization steps = 8,232\n","[INFO|trainer.py:1769] 2023-10-28 19:03:40,113 >>   Number of trainable parameters = 774,030,080\n","{'loss': 3.0838, 'learning_rate': 9.395043731778425e-07, 'epoch': 0.18}\n"," 10% 800/8232 [36:55<5:43:14,  2.77s/it][INFO|trainer.py:2939] 2023-10-28 19:40:35,980 >> Saving model checkpoint to /content/drive/MyDrive/Colabs/result1/checkpoint-800\n","[INFO|configuration_utils.py:460] 2023-10-28 19:40:35,984 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-800/config.json\n","[INFO|configuration_utils.py:544] 2023-10-28 19:40:35,987 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-800/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-28 19:40:43,810 >> Model weights saved in /content/drive/MyDrive/Colabs/result1/checkpoint-800/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2420] 2023-10-28 19:40:43,814 >> tokenizer config file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-800/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2429] 2023-10-28 19:40:43,817 >> Special tokens file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-800/special_tokens_map.json\n","{'loss': 2.363, 'learning_rate': 8.787657920310981e-07, 'epoch': 0.36}\n","{'loss': 2.1997, 'learning_rate': 8.180272108843537e-07, 'epoch': 0.55}\n"," 19% 1600/8232 [1:14:17<5:05:38,  2.77s/it][INFO|trainer.py:2939] 2023-10-28 20:17:57,603 >> Saving model checkpoint to /content/drive/MyDrive/Colabs/result1/checkpoint-1600\n","[INFO|configuration_utils.py:460] 2023-10-28 20:17:57,608 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-1600/config.json\n","[INFO|configuration_utils.py:544] 2023-10-28 20:17:57,612 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-1600/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-28 20:18:07,713 >> Model weights saved in /content/drive/MyDrive/Colabs/result1/checkpoint-1600/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2420] 2023-10-28 20:18:07,719 >> tokenizer config file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-1600/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2429] 2023-10-28 20:18:07,721 >> Special tokens file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-1600/special_tokens_map.json\n","{'loss': 2.1196, 'learning_rate': 7.572886297376093e-07, 'epoch': 0.73}\n"," 29% 2400/8232 [1:51:49<4:28:48,  2.77s/it][INFO|trainer.py:2939] 2023-10-28 20:55:29,269 >> Saving model checkpoint to /content/drive/MyDrive/Colabs/result1/checkpoint-2400\n","[INFO|configuration_utils.py:460] 2023-10-28 20:55:29,274 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-2400/config.json\n","[INFO|configuration_utils.py:544] 2023-10-28 20:55:29,278 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-2400/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-28 20:55:36,875 >> Model weights saved in /content/drive/MyDrive/Colabs/result1/checkpoint-2400/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2420] 2023-10-28 20:55:36,879 >> tokenizer config file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-2400/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2429] 2023-10-28 20:55:36,882 >> Special tokens file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-2400/special_tokens_map.json\n","[INFO|trainer.py:3026] 2023-10-28 20:55:59,425 >> Deleting older checkpoint [/content/drive/MyDrive/Colabs/result1/checkpoint-800] due to args.save_total_limit\n","{'loss': 2.0768, 'learning_rate': 6.965500485908649e-07, 'epoch': 0.91}\n","{'loss': 2.0453, 'learning_rate': 6.35932944606414e-07, 'epoch': 1.09}\n"," 39% 3200/8232 [2:29:15<3:51:27,  2.76s/it][INFO|trainer.py:2939] 2023-10-28 21:32:55,422 >> Saving model checkpoint to /content/drive/MyDrive/Colabs/result1/checkpoint-3200\n","[INFO|configuration_utils.py:460] 2023-10-28 21:32:55,427 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-3200/config.json\n","[INFO|configuration_utils.py:544] 2023-10-28 21:32:55,430 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-3200/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-28 21:33:03,115 >> Model weights saved in /content/drive/MyDrive/Colabs/result1/checkpoint-3200/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2420] 2023-10-28 21:33:03,120 >> tokenizer config file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-3200/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2429] 2023-10-28 21:33:03,123 >> Special tokens file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-3200/special_tokens_map.json\n","[INFO|trainer.py:3026] 2023-10-28 21:33:20,982 >> Deleting older checkpoint [/content/drive/MyDrive/Colabs/result1/checkpoint-1600] due to args.save_total_limit\n","{'loss': 2.0277, 'learning_rate': 5.751943634596695e-07, 'epoch': 1.28}\n","{'loss': 2.01, 'learning_rate': 5.144557823129252e-07, 'epoch': 1.46}\n"," 49% 4000/8232 [3:06:42<3:15:24,  2.77s/it][INFO|trainer.py:2939] 2023-10-28 22:10:22,333 >> Saving model checkpoint to /content/drive/MyDrive/Colabs/result1/checkpoint-4000\n","[INFO|configuration_utils.py:460] 2023-10-28 22:10:22,337 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-4000/config.json\n","[INFO|configuration_utils.py:544] 2023-10-28 22:10:22,351 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-4000/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-28 22:10:29,597 >> Model weights saved in /content/drive/MyDrive/Colabs/result1/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2420] 2023-10-28 22:10:29,602 >> tokenizer config file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2429] 2023-10-28 22:10:29,605 >> Special tokens file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-4000/special_tokens_map.json\n","[INFO|trainer.py:3026] 2023-10-28 22:10:47,994 >> Deleting older checkpoint [/content/drive/MyDrive/Colabs/result1/checkpoint-2400] due to args.save_total_limit\n","{'loss': 1.9974, 'learning_rate': 4.5371720116618074e-07, 'epoch': 1.64}\n"," 58% 4800/8232 [3:44:05<2:37:56,  2.76s/it][INFO|trainer.py:2939] 2023-10-28 22:47:45,516 >> Saving model checkpoint to /content/drive/MyDrive/Colabs/result1/checkpoint-4800\n","[INFO|configuration_utils.py:460] 2023-10-28 22:47:45,520 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-4800/config.json\n","[INFO|configuration_utils.py:544] 2023-10-28 22:47:45,524 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-4800/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-28 22:47:53,269 >> Model weights saved in /content/drive/MyDrive/Colabs/result1/checkpoint-4800/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2420] 2023-10-28 22:47:53,273 >> tokenizer config file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-4800/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2429] 2023-10-28 22:47:56,116 >> Special tokens file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-4800/special_tokens_map.json\n","[INFO|trainer.py:3026] 2023-10-28 22:48:13,015 >> Deleting older checkpoint [/content/drive/MyDrive/Colabs/result1/checkpoint-3200] due to args.save_total_limit\n","{'loss': 1.9848, 'learning_rate': 3.929786200194363e-07, 'epoch': 1.82}\n"," 67% 5500/8232 [4:16:51<2:06:06,  2.77s/it]Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Colabs/run_clm.py\", line 662, in <module>\n","    main()\n","  File \"/content/drive/MyDrive/Colabs/run_clm.py\", line 610, in main\n","    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1591, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1984, in _inner_training_loop\n","    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2314, in _maybe_log_save_evaluate\n","    self.log(logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2692, in log\n","    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 399, in on_log\n","    return self.call_event(\"on_log\", args, state, control, logs=logs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 406, in call_event\n","    result = getattr(callback, event)(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 657, in on_log\n","    self.tb_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 1233, in flush\n","    writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 146, in flush\n","    self.event_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 125, in flush\n","    self._async_writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 190, in flush\n","    self._writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/record_writer.py\", line 43, in flush\n","    self._writer.flush()\n","  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\", line 221, in flush\n","    self._writable_file.flush()\n","tensorflow.python.framework.errors_impl.FailedPreconditionError: /content/drive/MyDrive/Colabs/result1/runs/Oct28_19-00-20_d6cc36b03fe1/events.out.tfevents.1698519820.d6cc36b03fe1.1944.0; Transport endpoint is not connected\n"," 67% 5500/8232 [4:16:51<2:07:35,  2.80s/it]\n","I0000 00:00:1698535232.452784    1944 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.\n"]}]},{"cell_type":"code","source":["drive.mount(\"/content/drive\", force_remount=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxjDQCICnQwj","executionInfo":{"status":"ok","timestamp":1698535649153,"user_tz":240,"elapsed":3171,"user":{"displayName":"wfls96","userId":"04099767878386371096"}},"outputId":"de7d0e0f-726c-4a60-c7de-282ab3b636d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!python '/content/drive/MyDrive/Colabs/run_clm.py' --model_name_or_path nferruz/ProtGPT2 \\\n","    --resume_from_checkpoint '/content/drive/MyDrive/Colabs/result1/checkpoint-4800' \\\n","    --train_file '/content/drive/MyDrive/Colabs/training_small.txt' \\\n","    --validation_file '/content/drive/MyDrive/Colabs/validation_small.txt' \\\n","    --tokenizer_name nferruz/ProtGPT2 \\\n","    --do_train \\\n","    --do_eval  \\\n","    --output_dir '/content/drive/MyDrive/Colabs/result1' \\\n","    --per_device_train_batch_size 1 \\\n","    --per_device_eval_batch_size 1 \\\n","    --gradient_accumulation_steps=16 \\\n","    --fp16 \\\n","    --learning_rate 1e-06 \\\n","    --save_steps 800 \\\n","    --save_total_limit 2\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dq5UvWpGnSzo","executionInfo":{"status":"ok","timestamp":1698546077023,"user_tz":240,"elapsed":10280350,"user":{"displayName":"wfls96","userId":"04099767878386371096"}},"outputId":"9e8f9b43-3816-4dc4-b597-081d5ce5fb57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-10-28 23:29:59.782269: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-10-28 23:29:59.782324: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-10-28 23:29:59.782354: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-10-28 23:30:00.954066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1698535803.335056   70234 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n","10/28/2023 23:30:05 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n","10/28/2023 23:30:05 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=True,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=16,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=1e-06,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Colabs/result1/runs/Oct28_23-30-03_d6cc36b03fe1,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_torch,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/Colabs/result1,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=1,\n","per_device_train_batch_size=1,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=/content/drive/MyDrive/Colabs/result1/checkpoint-4800,\n","run_name=/content/drive/MyDrive/Colabs/result1,\n","save_on_each_node=False,\n","save_safetensors=False,\n","save_steps=800,\n","save_strategy=steps,\n","save_total_limit=2,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-1d41c0d41954827a\n","10/28/2023 23:30:06 - INFO - datasets.builder - Using custom data configuration default-1d41c0d41954827a\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n","10/28/2023 23:30:06 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n","Overwrite dataset info from restored data version if exists.\n","10/28/2023 23:30:06 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34\n","10/28/2023 23:30:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34\n","Found cached dataset text (/root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)\n","10/28/2023 23:30:06 - INFO - datasets.builder - Found cached dataset text (/root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)\n","Loading Dataset info from /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34\n","10/28/2023 23:30:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34\n","[INFO|configuration_utils.py:715] 2023-10-28 23:30:06,864 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/config.json\n","[INFO|configuration_utils.py:775] 2023-10-28 23:30:06,865 >> Model config GPT2Config {\n","  \"_name_or_path\": \"nferruz/ProtGPT2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 0,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1280,\n","  \"n_head\": 20,\n","  \"n_inner\": null,\n","  \"n_layer\": 36,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.34.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","[INFO|tokenization_auto.py:550] 2023-10-28 23:30:07,109 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:715] 2023-10-28 23:30:07,364 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/config.json\n","[INFO|configuration_utils.py:775] 2023-10-28 23:30:07,365 >> Model config GPT2Config {\n","  \"_name_or_path\": \"nferruz/ProtGPT2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 0,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1280,\n","  \"n_head\": 20,\n","  \"n_inner\": null,\n","  \"n_layer\": 36,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.34.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 23:30:07,867 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/vocab.json\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 23:30:07,867 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/merges.txt\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 23:30:07,867 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/tokenizer.json\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 23:30:07,867 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 23:30:07,867 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2015] 2023-10-28 23:30:07,867 >> loading file tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:715] 2023-10-28 23:30:07,867 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/config.json\n","[INFO|configuration_utils.py:775] 2023-10-28 23:30:07,868 >> Model config GPT2Config {\n","  \"_name_or_path\": \"nferruz/ProtGPT2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 0,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1280,\n","  \"n_head\": 20,\n","  \"n_inner\": null,\n","  \"n_layer\": 36,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.34.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","[INFO|modeling_utils.py:2993] 2023-10-28 23:30:07,936 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/pytorch_model.bin\n","[INFO|configuration_utils.py:770] 2023-10-28 23:30:09,721 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 0\n","}\n","\n","[INFO|modeling_utils.py:3775] 2023-10-28 23:30:17,429 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","[INFO|modeling_utils.py:3783] 2023-10-28 23:30:17,429 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at nferruz/ProtGPT2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","[INFO|modeling_utils.py:3352] 2023-10-28 23:30:17,681 >> Generation config file not found, using a generation config created from the model config.\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-ab081c8bbbd29204.arrow\n","10/28/2023 23:30:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-ab081c8bbbd29204.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-332caaa9ba9261ca.arrow\n","10/28/2023 23:30:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-332caaa9ba9261ca.arrow\n","10/28/2023 23:30:17 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Using block_size=1024 instead. You can change that default value by passing --block_size xxx.\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-19cc271c971a97fa.arrow\n","10/28/2023 23:30:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-19cc271c971a97fa.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-74f973b5d0ce3073.arrow\n","10/28/2023 23:30:17 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-1d41c0d41954827a/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-74f973b5d0ce3073.arrow\n","[INFO|trainer.py:2123] 2023-10-28 23:30:20,497 >> Loading model from /content/drive/MyDrive/Colabs/result1/checkpoint-4800.\n","[INFO|trainer.py:1760] 2023-10-28 23:30:30,612 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-28 23:30:30,612 >>   Num examples = 43,907\n","[INFO|trainer.py:1762] 2023-10-28 23:30:30,612 >>   Num Epochs = 3\n","[INFO|trainer.py:1763] 2023-10-28 23:30:30,612 >>   Instantaneous batch size per device = 1\n","[INFO|trainer.py:1766] 2023-10-28 23:30:30,612 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1767] 2023-10-28 23:30:30,612 >>   Gradient Accumulation steps = 16\n","[INFO|trainer.py:1768] 2023-10-28 23:30:30,612 >>   Total optimization steps = 8,232\n","[INFO|trainer.py:1769] 2023-10-28 23:30:30,614 >>   Number of trainable parameters = 774,030,080\n","[INFO|trainer.py:1789] 2023-10-28 23:30:30,615 >>   Continuing training from checkpoint, will skip to saved global_step\n","[INFO|trainer.py:1790] 2023-10-28 23:30:30,615 >>   Continuing training from epoch 1\n","[INFO|trainer.py:1791] 2023-10-28 23:30:30,615 >>   Continuing training from global step 4800\n","[INFO|trainer.py:1793] 2023-10-28 23:30:30,615 >>   Will skip the first 1 epochs then the first 32896 batches in the first epoch.\n","{'loss': 1.9814, 'learning_rate': 3.929786200194363e-07, 'epoch': 1.82}\n","{'loss': 1.973, 'learning_rate': 3.322400388726919e-07, 'epoch': 2.0}\n"," 68% 5600/8232 [37:09<2:02:19,  2.79s/it][INFO|trainer.py:2939] 2023-10-29 00:07:40,274 >> Saving model checkpoint to /content/drive/MyDrive/Colabs/result1/checkpoint-5600\n","[INFO|configuration_utils.py:460] 2023-10-29 00:07:40,279 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-5600/config.json\n","[INFO|configuration_utils.py:544] 2023-10-29 00:07:40,283 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-5600/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-29 00:07:50,591 >> Model weights saved in /content/drive/MyDrive/Colabs/result1/checkpoint-5600/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2420] 2023-10-29 00:07:50,596 >> tokenizer config file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-5600/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2429] 2023-10-29 00:07:50,599 >> Special tokens file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-5600/special_tokens_map.json\n","[INFO|trainer.py:3026] 2023-10-29 00:08:13,968 >> Deleting older checkpoint [/content/drive/MyDrive/Colabs/result1/checkpoint-4000] due to args.save_total_limit\n","{'loss': 1.9679, 'learning_rate': 2.7150145772594753e-07, 'epoch': 2.19}\n"," 78% 6400/8232 [1:14:56<1:24:46,  2.78s/it][INFO|trainer.py:2939] 2023-10-29 00:45:27,501 >> Saving model checkpoint to /content/drive/MyDrive/Colabs/result1/checkpoint-6400\n","[INFO|configuration_utils.py:460] 2023-10-29 00:45:27,506 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-6400/config.json\n","[INFO|configuration_utils.py:544] 2023-10-29 00:45:27,510 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-6400/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-29 00:45:38,485 >> Model weights saved in /content/drive/MyDrive/Colabs/result1/checkpoint-6400/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2420] 2023-10-29 00:45:38,490 >> tokenizer config file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-6400/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2429] 2023-10-29 00:45:38,493 >> Special tokens file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-6400/special_tokens_map.json\n","[INFO|trainer.py:3026] 2023-10-29 00:45:59,716 >> Deleting older checkpoint [/content/drive/MyDrive/Colabs/result1/checkpoint-4800] due to args.save_total_limit\n","{'loss': 1.9615, 'learning_rate': 2.108843537414966e-07, 'epoch': 2.37}\n","{'loss': 1.9569, 'learning_rate': 1.5014577259475217e-07, 'epoch': 2.55}\n"," 87% 7200/8232 [1:52:35<47:39,  2.77s/it][INFO|trainer.py:2939] 2023-10-29 01:23:06,107 >> Saving model checkpoint to /content/drive/MyDrive/Colabs/result1/checkpoint-7200\n","[INFO|configuration_utils.py:460] 2023-10-29 01:23:06,112 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-7200/config.json\n","[INFO|configuration_utils.py:544] 2023-10-29 01:23:06,115 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-7200/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-29 01:23:14,397 >> Model weights saved in /content/drive/MyDrive/Colabs/result1/checkpoint-7200/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2420] 2023-10-29 01:23:14,402 >> tokenizer config file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-7200/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2429] 2023-10-29 01:23:14,404 >> Special tokens file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-7200/special_tokens_map.json\n","[INFO|trainer.py:3026] 2023-10-29 01:23:39,875 >> Deleting older checkpoint [/content/drive/MyDrive/Colabs/result1/checkpoint-5600] due to args.save_total_limit\n","{'loss': 1.9544, 'learning_rate': 8.940719144800778e-08, 'epoch': 2.73}\n","{'loss': 1.9524, 'learning_rate': 2.866861030126336e-08, 'epoch': 2.92}\n"," 97% 8000/8232 [2:30:15<10:48,  2.80s/it][INFO|trainer.py:2939] 2023-10-29 02:00:46,076 >> Saving model checkpoint to /content/drive/MyDrive/Colabs/result1/checkpoint-8000\n","[INFO|configuration_utils.py:460] 2023-10-29 02:00:46,080 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-8000/config.json\n","[INFO|configuration_utils.py:544] 2023-10-29 02:00:46,084 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/checkpoint-8000/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-29 02:00:55,468 >> Model weights saved in /content/drive/MyDrive/Colabs/result1/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2420] 2023-10-29 02:00:55,473 >> tokenizer config file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2429] 2023-10-29 02:00:55,476 >> Special tokens file saved in /content/drive/MyDrive/Colabs/result1/checkpoint-8000/special_tokens_map.json\n","[INFO|trainer.py:3026] 2023-10-29 02:01:14,218 >> Deleting older checkpoint [/content/drive/MyDrive/Colabs/result1/checkpoint-6400] due to args.save_total_limit\n","100% 8232/8232 [2:41:36<00:00,  2.80s/it][INFO|trainer.py:2017] 2023-10-29 02:12:07,400 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 9696.8193, 'train_samples_per_second': 13.584, 'train_steps_per_second': 0.849, 'train_loss': 0.8178972331381632, 'epoch': 3.0}\n","100% 8232/8232 [2:41:36<00:00,  1.18s/it]\n","[INFO|trainer.py:2939] 2023-10-29 02:12:07,439 >> Saving model checkpoint to /content/drive/MyDrive/Colabs/result1\n","[INFO|configuration_utils.py:460] 2023-10-29 02:12:07,444 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/config.json\n","[INFO|configuration_utils.py:544] 2023-10-29 02:12:07,448 >> Configuration saved in /content/drive/MyDrive/Colabs/result1/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-29 02:12:21,675 >> Model weights saved in /content/drive/MyDrive/Colabs/result1/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2420] 2023-10-29 02:12:21,680 >> tokenizer config file saved in /content/drive/MyDrive/Colabs/result1/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2429] 2023-10-29 02:12:21,683 >> Special tokens file saved in /content/drive/MyDrive/Colabs/result1/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =     0.8179\n","  train_runtime            = 2:41:36.81\n","  train_samples            =      43907\n","  train_samples_per_second =     13.584\n","  train_steps_per_second   =      0.849\n","10/29/2023 02:12:25 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:3213] 2023-10-29 02:12:25,355 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-29 02:12:25,355 >>   Num examples = 7757\n","[INFO|trainer.py:3218] 2023-10-29 02:12:25,355 >>   Batch size = 1\n","100% 7757/7757 [08:48<00:00, 14.68it/s]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_accuracy           =      0.673\n","  eval_loss               =      1.914\n","  eval_runtime            = 0:08:48.51\n","  eval_samples            =       7757\n","  eval_samples_per_second =     14.677\n","  eval_steps_per_second   =     14.677\n","  perplexity              =     6.7803\n","[INFO|modelcard.py:452] 2023-10-29 02:21:14,262 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.6730159282235035}]}\n","I0000 00:00:1698546074.983594   70234 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.\n"]}]}]}